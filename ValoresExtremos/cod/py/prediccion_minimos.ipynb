{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eb0733",
   "metadata": {},
   "source": [
    "## Objetivo del Notebook\n",
    "\n",
    "Se quiere mejorar el modelo antiguo de predicción para el cuadl míñimo, este modelo se basaba en predecir el comportamiento de de los mínimos mensuales, sin emabrgo lo que interesa es predecir el valor mínimo de esta serie de tiempo por lo que se cambiará la metodología a una que involucre Teoría de Valores Extremos (EVT).\n",
    "\n",
    "### Librerías Necesarias\n",
    "\n",
    "En este caso se utilizará PyTorch para la creación del modelo Gated Recurrent Units (GRU) aprovechando del GPU para acelerar el proceso mediante CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a6b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerias\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2b9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "## Activar GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6e73f",
   "metadata": {},
   "source": [
    "### Metodología\n",
    "\n",
    "Se usará la metodología de Teoría de Valores Extremos propuesta en el paper Modeling Extreme Values in Time Series sin embargo se usará únicamente el umbral del percentil 5% para predecir únicamente los mínimos. \n",
    "Se harán clases internas y funciones para facilitar la creación del modelo y su posterior entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e9992",
   "metadata": {},
   "source": [
    "### Clase = Serie_Tiempo\n",
    "\n",
    "El objetivo de esta clase es la manipulación de la serie de tiempo, tal y como se menciona en el paper, donde se establecen los parámetros como el umbral, la base de datos, el tamaño de la ventana y se crea la sucesión de sucesiones para los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61691a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Serie_Tiempo(Dataset):\n",
    "\n",
    "    def __init__(self, data, tamano_ventana, pasos_prediccion, umbral, train = True):\n",
    "\n",
    "        self.data = data\n",
    "        self.tamano_ventana = tamano_ventana\n",
    "        self.pasos_prediccion = pasos\n",
    "        self.umbral = umbral\n",
    "        self.train = train\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        if self.train:\n",
    "            \n",
    "            self.scaler.fit(self.data)\n",
    "        \n",
    "        self.data_normalizada = self.scaler.transform(data)\n",
    "        self.X, self.Y, self.V = self.crear_sucesiones()\n",
    "\n",
    "    def crear_sucesiones(self):\n",
    "\n",
    "        X, Y, V = [], [], []\n",
    "\n",
    "        for i in range(len(self.data_normalizada) - self.tamano_ventana - self.pasos_prediccion):\n",
    "\n",
    "            X_sucesion = self.data_normalizad[i:i+self.tamano_ventana]\n",
    "            Y_predecir = self.data_normalizada[i+self.tamano_ventana + self.pasos_prediccion -1]\n",
    "            V_extremo = 1 if Y_predecir < -self.umbral else 0\n",
    "\n",
    "            X.append(X_sucesion)\n",
    "            Y.append(Y_predecir)\n",
    "            V.append(V_extremo)\n",
    "\n",
    "        return np.array(X), np.array(Y), np.array(V)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.X[idx]).to(device),\n",
    "                torch.FloatTensor([self.y[idx]]).to(device),\n",
    "                torch.FloatTensor([self.v[idx]]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef536004",
   "metadata": {},
   "source": [
    "### Clase = GatedRecurrentUnits\n",
    "\n",
    "Como se menciona en el paper, se usará un modelo GRU para la predicción de los valores y mediante un índice de pérdida (definido en la anterior clase) se mejorará el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44efdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedRecurrentUnits(nn.Module):\n",
    "\n",
    "    def __init__(self, tamano_entrada, tamano_oculto, tamano_memoria, tamano_ventanas, gamma = 2.0):\n",
    "\n",
    "        super(GatedRecurrentUnits, self).__init__()\n",
    "\n",
    "        self.tamano_oculto = tamano_oculto\n",
    "        self.tamano_memoria = tamano_memoria\n",
    "        self.gamma = gamma\n",
    "        self.tamano_ventanas = tamano_ventanas\n",
    "\n",
    "        self.gru = nn.GRU(input_size = tamano_entrada, hidden_size = tamano_oculto, batch_first = True, num_layers = 2, dropout  = 0.2)\n",
    "\n",
    "        self.memoria_gru = nn.GRU(input_size = tamano_entrada, hidden_size = tamano_oculto, batch_first = True)\n",
    "\n",
    "        self.memoria_S = nn.Parameter(torch.randn(tamano_memoria, tamano_oculto)*0.1)\n",
    "        self.memoria_Q = nn.Parameter(torch.zeros(tamano_memoria))\n",
    "\n",
    "        self.pesos = nn.Linear(tamano_oculto, tamano_oculto)\n",
    "        self.capa_salida = nn.Linear(tamano_oculto, 1)\n",
    "        self.parametro_escala = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.evento_predictor = nn.Sequential(\n",
    "            nn.Linear(tamano_oculto, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self._inicializar_pesos()\n",
    "\n",
    "    def _inicializar_pesos(self):\n",
    "\n",
    "        for nombre, param in self.named_parameters():\n",
    "\n",
    "            if 'weight' in nombre:\n",
    "                nn.init.xavier_unifoirm_(param)\n",
    "            elif 'bias' in nombre:\n",
    "                nn.init.constant_(param,0)\n",
    "\n",
    "    def forward(self, x, memoria_actualizar = False, ventanas_memoria = None):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        gru_out, _ = self.gru(x)\n",
    "        estado_oculto = gru_out[:, -1, :]\n",
    "\n",
    "        base_pred = self.capa_salida(estado_oculto)\n",
    "\n",
    "        scores_atencion = torch.matmul(estado_oculto, self.memoria_S.t())\n",
    "        pesos_atencion = torch.softmax(scores_atencion, dim=-1)\n",
    "\n",
    "        prediccion_evento = torch.matmul(pesos_atencion, torch.sigmoid(self.memoria_Q))\n",
    "        prediccion_evento = torch.sigmoid(prediccion_evento)\n",
    "\n",
    "        prediccion_final = base_pred - self.parametro_escala * prediccion_evento\n",
    "\n",
    "        ventanas_memoria = None\n",
    "        if ventanas_memoria is not None:\n",
    "\n",
    "            ventanas_features = []\n",
    "\n",
    "            for ventana in ventanas_memoria:\n",
    "\n",
    "                _, ventana_oculta = self.memoria_gru(ventana.unsqueeze(0))\n",
    "                ventanas_features.append(ventana_oculta.squeeze(0))\n",
    "\n",
    "            ventanas_features = torch.stack(ventanas_features)\n",
    "            ventanas_prediccion = self.evento_predictor(ventanas_features)\n",
    "        \n",
    "        return prediccion_final, base_pred, evento_predictor, ventanas_prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extreme_value_loss(u, v, gamma=2.0, beta0=0.9, beta1=0.1):\n",
    "    \"\"\"\n",
    "    Pérdida para eventos extremos de mínimo\n",
    "    u: predicciones de probabilidad de evento\n",
    "    v: etiquetas reales (0 o 1)\n",
    "    \"\"\"\n",
    "    # Término de peso basado en Extreme Value Theory\n",
    "    weight_term = torch.clamp(1 - u / gamma, min=1e-8) ** gamma\n",
    "    \n",
    "    # Pérdida para eventos positivos (mínimos extremos)\n",
    "    loss_positive = -beta0 * weight_term * v * torch.log(u + 1e-8)\n",
    "    \n",
    "    # Pérdida para eventos negativos (normales)\n",
    "    loss_negative = -beta1 * (1 - v) * torch.log(1 - u + 1e-8)\n",
    "    \n",
    "    return (loss_positive + loss_negative).mean()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (X_batch, y_batch, v_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            final_pred, base_pred, event_pred, _ = model(X_batch)\n",
    "            \n",
    "            # Calcular pérdidas\n",
    "            loss_mse = mse_loss(final_pred.squeeze(), y_batch.squeeze())\n",
    "            loss_evl = extreme_value_loss(event_pred.squeeze(), v_batch.squeeze())\n",
    "            \n",
    "            # Pérdida total\n",
    "            loss = loss_mse + 0.5 * loss_evl\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets, val_events = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, v_batch in val_loader:\n",
    "                final_pred, base_pred, event_pred, _ = model(X_batch)\n",
    "                \n",
    "                loss_mse = mse_loss(final_pred.squeeze(), y_batch.squeeze())\n",
    "                loss_evl = extreme_value_loss(event_pred.squeeze(), v_batch.squeeze())\n",
    "                loss = loss_mse + 0.5 * loss_evl\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(final_pred.cpu().numpy())\n",
    "                val_targets.extend(y_batch.cpu().numpy())\n",
    "                val_events.extend((event_pred > 0.5).cpu().numpy().astype(int))\n",
    "        \n",
    "        # Calcular métricas\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "        \n",
    "        # Calcular F1-score para eventos extremos\n",
    "        val_true_events = [1 if target < -0.5 else 0 for target in val_targets]  # Asumiendo epsilon=0.5\n",
    "        val_f1 = f1_score(val_true_events, val_events, zero_division=0)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch:3d}/{num_epochs}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Parámetros de configuración\n",
    "config = {\n",
    "    'window_size': 50,\n",
    "    'prediction_steps': 1,\n",
    "    'hidden_size': 128,\n",
    "    'memory_size': 80,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 0.0005,\n",
    "    'epsilon': 0.5,  # Umbral para mínimos extremos (ajustar según tus datos)\n",
    "    'gamma': 2.0,    # Parámetro EVT\n",
    "}\n",
    "\n",
    "# Ejemplo de uso\n",
    "def main():\n",
    "    # 1. Cargar tus datos (reemplaza con tus datos reales)\n",
    "    # data = pd.read_csv('tu_archivo.csv')['caudal'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Datos de ejemplo (simulados)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    time = np.linspace(0, 20, n_samples)\n",
    "    data = np.sin(time) + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Añadir algunos mínimos extremos\n",
    "    extreme_indices = [200, 450, 700, 850]\n",
    "    data[extreme_indices] -= 2.5  # Mínimos extremos\n",
    "    \n",
    "    data = data.reshape(-1, 1)\n",
    "    \n",
    "    # 2. Dividir datos\n",
    "    train_size = int(0.7 * len(data))\n",
    "    val_size = int(0.15 * len(data))\n",
    "    \n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "    \n",
    "    # 3. Crear datasets y dataloaders\n",
    "    train_dataset = TimeSeriesDataset(train_data, config['window_size'], \n",
    "                                    config['prediction_steps'], config['epsilon'], train=True)\n",
    "    val_dataset = TimeSeriesDataset(val_data, config['window_size'], \n",
    "                                  config['prediction_steps'], config['epsilon'], train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # 4. Crear modelo\n",
    "    model = ExtremeMemoryNetwork(\n",
    "        input_size=1,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        memory_size=config['memory_size'],\n",
    "        window_size=config['window_size'],\n",
    "        gamma=config['gamma']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f'Número de parámetros: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    \n",
    "    # 5. Entrenar modelo\n",
    "    print(\"Comenzando entrenamiento...\")\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, \n",
    "        config['num_epochs'], config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # 6. Evaluar y visualizar resultados\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    # ... (código para evaluación y visualización)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
